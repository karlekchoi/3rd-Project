from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoModel, AutoTokenizer
import io
import base64
import logging
import whisper
import tempfile
import os
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import requests
from dotenv import load_dotenv
import easyocr
import google.generativeai as genai

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder='public', static_url_path='')

# CORS ì„¤ì • - ëª¨ë“  ì¶œì²˜ í—ˆìš©
CORS(app, resources={
    r"/*": {
        "origins": "*",
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# CORS í—¤ë”ë¥¼ ëª¨ë“  ì‘ë‹µì— ì¶”ê°€
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥ (í•œ ë²ˆë§Œ ë¡œë“œ)
processor = None
model = None
whisper_model = None
sroberta_model = None
sroberta_tokenizer = None
easyocr_reader = None

# Static íŒŒì¼ (ë²ˆì—­ íŒŒì¼) ì„œë¹™ ë¼ìš°íŠ¸
from flask import send_from_directory

@app.route('/locales/<path:filename>')
def serve_locales(filename):
    """ë²ˆì—­ íŒŒì¼ ì„œë¹™"""
    return send_from_directory('public/locales', filename)

def load_model():
    """TrOCR ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global processor, model
    
    if processor is None or model is None:
        logger.info("TrOCR ëª¨ë¸ ë¡œë”© ì¤‘...")
        processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')
        model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if torch.cuda.is_available():
            model = model.to('cuda')
            logger.info("GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logger.info("CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        logger.info("TrOCR ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return processor, model

def load_easyocr_reader():
    """EasyOCR Reader ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global easyocr_reader
    
    if easyocr_reader is None:
        logger.info("EasyOCR í•œê¸€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        # í•œê¸€ê³¼ ì˜ì–´ë¥¼ ì§€ì›í•˜ëŠ” Reader ìƒì„±
        easyocr_reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())
        logger.info("EasyOCR ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return easyocr_reader

def load_whisper_model():
    """Whisper ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("Whisper Small ëª¨ë¸ ë¡œë”© ì¤‘...")
        whisper_model = whisper.load_model("small")  # small, medium, large ì¤‘ ì„ íƒ
        logger.info("Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return whisper_model

def load_sroberta_model():
    """ko-sroberta-multitask ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global sroberta_model, sroberta_tokenizer
    
    if sroberta_model is None or sroberta_tokenizer is None:
        logger.info("ko-sroberta-multitask ëª¨ë¸ ë¡œë”© ì¤‘...")
        sroberta_tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
        sroberta_model = AutoModel.from_pretrained("jhgan/ko-sroberta-multitask")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if torch.cuda.is_available():
            sroberta_model = sroberta_model.to('cuda')
            logger.info("ko-sroberta: GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logger.info("ko-sroberta: CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        sroberta_model.eval()  # í‰ê°€ ëª¨ë“œ
        logger.info("ko-sroberta-multitask ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return sroberta_model, sroberta_tokenizer

def mean_pooling(model_output, attention_mask):
    """Mean Pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê·  ê³„ì‚°"""
    token_embeddings = model_output[0]  # ëª¨ë“  í† í° ì„ë² ë”©
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def get_sentence_embedding(text):
    """
    ë¬¸ì¥ ì„ë² ë”© ìƒì„± (ko-sroberta-multitask + Mean Pooling)
    
    Args:
        text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
    
    Returns:
        numpy array: ë¬¸ì¥ ì„ë² ë”©
    """
    logger.info("ğŸ¯ ko-sroberta-multitaskë¡œ ì„ë² ë”© ìƒì„±")
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_sroberta_model()
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°)
    if isinstance(text, str):
        text = [text]
    
    # í† í¬ë‚˜ì´ì§•
    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')
    
    # GPU ì‚¬ìš© ì‹œ í…ì„œë„ GPUë¡œ
    if torch.cuda.is_available():
        encoded_input = {k: v.to('cuda') for k, v in encoded_input.items()}
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        model_output = model(**encoded_input)
    
    # Mean Pooling
    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
    
    # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
    embeddings = sentence_embeddings.cpu().numpy()
    
    logger.info("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    return embeddings

@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "ok",
        "models": {
            "trocr": "microsoft/trocr-base-handwritten",
            "whisper": "openai/whisper-small",
            "sroberta": "jhgan/ko-sroberta-multitask"
        },
        "similarity_model": "ko-sroberta-multitask (Mean Pooling)",
        "accuracy": "92%+",
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    })

@app.route('/recognize', methods=['POST'])
def recognize_handwriting():
    """ì†ê¸€ì”¨ ì¸ì‹ API - EasyOCR ì‚¬ìš© (ì „ì²˜ë¦¬ ê°•í™”)"""
    try:
        # ìš”ì²­ ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        image_base64 = data['image']
        
        # data:image/png;base64, ë¶€ë¶„ ì œê±°
        if ',' in image_base64:
            image_base64 = image_base64.split(',')[1]
        
        image_bytes = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        logger.info(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê°„ë‹¨í•˜ê²Œ)
        # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        image_gray = image.convert('L')
        
        # 2. ì•½í•œ ëŒ€ë¹„ ì¦ê°€
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Contrast(image_gray)
        image_enhanced = enhancer.enhance(1.3)  # ëŒ€ë¹„ ì•½í•˜ê²Œ ì¦ê°€
        
        # 3. numpy ë°°ì—´ë¡œ ë³€í™˜
        image_np = np.array(image_enhanced)
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ - í¬ê¸°: {image_np.shape}")
        
        # EasyOCR Reader ë¡œë“œ
        reader = load_easyocr_reader()
        
        # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¸ì‹ (íŒŒë¼ë¯¸í„° ìµœì í™”)
        results = reader.readtext(
            image_np,
            detail=1,
            paragraph=False,
            min_size=5,  # ìµœì†Œ í…ìŠ¤íŠ¸ í¬ê¸° ë” ë‚®ì¶¤
            text_threshold=0.1,  # í…ìŠ¤íŠ¸ ê°ì§€ ì„ê³„ê°’ ë” ë‚®ì¶¤
            low_text=0.1,  # ë‚®ì€ í…ìŠ¤íŠ¸ ì ìˆ˜ ë” í—ˆìš©
            link_threshold=0.1,
            canvas_size=2560,
            mag_ratio=1.5,
            contrast_ths=0.1,  # ëŒ€ë¹„ ì„ê³„ê°’ ë‚®ì¶¤
            adjust_contrast=0.5  # ëŒ€ë¹„ ì¡°ì • ì•½í•˜ê²Œ
        )
        
        # ê²°ê³¼ ì²˜ë¦¬
        logger.info(f"EasyOCR ê²°ê³¼ ê°œìˆ˜: {len(results)}")
        
        if results:
            # ëª¨ë“  ê²°ê³¼ ë¡œê¹…
            for idx, (bbox, text, conf) in enumerate(results):
                logger.info(f"  ê²°ê³¼ {idx+1}: '{text}' (ì‹ ë¢°ë„: {conf:.2f})")
            
            # ì‹ ë¢°ë„ê°€ ê°€ì¥ ë†’ì€ ê²°ê³¼ ì„ íƒ
            best_result = max(results, key=lambda x: x[2])
            recognized_text = best_result[1]
            confidence = best_result[2]
            
            # í•œê¸€ë§Œ ì¶”ì¶œ (ìˆ«ì/ì˜ì–´ ì œê±°)
            import re
            korean_only = re.sub(r'[^ê°€-í£ã„±-ã…ã…-ã…£]', '', recognized_text)
            
            if korean_only:
                logger.info(f"âœ… ìµœì¢… ì¸ì‹: {korean_only} (ì‹ ë¢°ë„: {confidence:.2f})")
                return jsonify({
                    "text": korean_only,
                    "confidence": float(confidence),
                    "method": "easyocr",
                    "raw_text": recognized_text
                })
            else:
                logger.warning(f"âš ï¸ í•œê¸€ì´ ì—†ìŒ: {recognized_text}")
                return jsonify({
                    "text": recognized_text,  # í•œê¸€ì´ ì—†ì–´ë„ ì›ë³¸ ë°˜í™˜
                    "confidence": float(confidence),
                    "method": "easyocr"
                })
        else:
            logger.warning("âŒ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return jsonify({
                "text": "",
                "confidence": 0.0,
                "method": "easyocr",
                "message": "í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            })
    
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/recognize-mcp', methods=['POST'])
def recognize_handwriting_mcp():
    """MCPë¥¼ ì‚¬ìš©í•œ ì†ê¸€ì”¨ ì¸ì‹ API"""
    try:
        import subprocess
        import json as json_lib
        import os
        
        # ìš”ì²­ ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # Base64 ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        image_base64 = data['image']
        if ',' in image_base64:
            image_base64 = image_base64.split(',')[1]
        
        language = data.get('language', 'ko')
        
        logger.info("MCPë¥¼ ì‚¬ìš©í•œ ì†ê¸€ì”¨ ì¸ì‹ ì‹œì‘...")
        
        # Node.js MCP í”„ë¡ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        try:
            # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            current_dir = os.path.dirname(os.path.abspath(__file__))
            mcp_proxy_path = os.path.join(current_dir, 'mcp_proxy.js')
            
            # Node.jsë¥¼ í†µí•´ MCP í”„ë¡ì‹œ ì‹¤í–‰
            result = subprocess.run(
                ['node', mcp_proxy_path, image_base64, language],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=current_dir
            )
            
            if result.returncode != 0:
                error_output = result.stderr or result.stdout
                logger.warning(f"MCP í˜¸ì¶œ ì‹¤íŒ¨: {error_output}")
                # stderrì— ë¡œê·¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                try:
                    error_data = json_lib.loads(error_output)
                    raise Exception(error_data.get('error', 'MCP í˜¸ì¶œ ì‹¤íŒ¨'))
                except json_lib.JSONDecodeError:
                    raise Exception(f"MCP í˜¸ì¶œ ì‹¤íŒ¨: {error_output}")
            
            # ê²°ê³¼ íŒŒì‹± (stdoutì—ì„œ JSON ì½ê¸°)
            output_lines = result.stdout.strip().split('\n')
            # ë§ˆì§€ë§‰ JSON ë¼ì¸ ì°¾ê¸° (ë¡œê·¸ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìŒ)
            json_output = None
            for line in reversed(output_lines):
                line = line.strip()
                if line.startswith('{') and line.endswith('}'):
                    try:
                        json_output = json_lib.loads(line)
                        break
                    except json_lib.JSONDecodeError:
                        continue
            
            if not json_output:
                # ì „ì²´ stdoutì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
                try:
                    json_output = json_lib.loads(result.stdout)
                except json_lib.JSONDecodeError:
                    raise Exception(f"MCP ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {result.stdout}")
            
            recognized_text = json_output.get('text', json_output.get('result', ''))
            
            if not recognized_text:
                logger.warning(f"MCP ê²°ê³¼ì— textê°€ ì—†ìŒ: {json_output}")
                raise Exception("MCPì—ì„œ ì¸ì‹ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"MCP ì¸ì‹ ê²°ê³¼: {recognized_text}")
            
            return jsonify({
                "text": recognized_text,
                "method": "mcp"
            })
            
        except subprocess.TimeoutExpired:
            logger.error("MCP í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
            raise Exception("MCP í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)")
        except json_lib.JSONDecodeError as e:
            logger.error(f"MCP ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}, stdout: {result.stdout if 'result' in locals() else 'N/A'}")
            raise Exception(f"MCP ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"MCP í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise
    
    except Exception as e:
        logger.error(f"MCP ì†ê¸€ì”¨ ì¸ì‹ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/transcribe', methods=['POST'])
def transcribe_audio():
    """Whisper ìŒì„± ì¸ì‹ API"""
    try:
        # íŒŒì¼ ë°›ê¸°
        if 'file' not in request.files:
            return jsonify({"error": "ì˜¤ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        audio_file = request.files['file']
        
        if audio_file.filename == '':
            return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400
        
        logger.info(f"ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì‹ : {audio_file.filename}, íƒ€ì…: {audio_file.content_type}")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.webm') as temp_file:
            audio_file.save(temp_file.name)
            temp_path = temp_file.name
        
        try:
            # Whisper ëª¨ë¸ ë¡œë“œ
            model = load_whisper_model()
            
            # ìŒì„± ì¸ì‹ (í•œêµ­ì–´ë¡œ ëª…ì‹œ)
            logger.info("Whisperë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
            result = model.transcribe(temp_path, language='ko', fp16=False)
            
            transcription = result['text'].strip()
            logger.info(f"ì¸ì‹ ê²°ê³¼: {transcription}")
            
            return jsonify({
                "text": transcription,
                "language": "ko"
            })
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.unlink(temp_path)
    
    except Exception as e:
        logger.error(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/similarity', methods=['POST'])
def calculate_similarity():
    """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° API (ì£¼ê´€ì‹ ë‹µë³€ ì±„ì ìš©)"""
    try:
        data = request.get_json()
        
        if not data or 'text1' not in data or 'text2' not in data:
            return jsonify({"error": "text1ê³¼ text2ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text1 = data['text1']
        text2 = data['text2']
        
        logger.info(f"ìœ ì‚¬ë„ ê³„ì‚°: '{text1}' vs '{text2}'")
        
        # ì„ë² ë”© ìƒì„±
        emb1 = get_sentence_embedding(text1)
        emb2 = get_sentence_embedding(text2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(emb1, emb2)[0][0]
        similarity_percent = float(similarity * 100)
        
        logger.info(f"ìœ ì‚¬ë„: {similarity_percent:.2f}%")
        
        return jsonify({
            "similarity": similarity_percent,
            "text1": text1,
            "text2": text2,
            "is_similar": similarity_percent >= 70  # 70% ì´ìƒì´ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        })
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/similar_words', methods=['POST'])
def find_similar_words():
    """ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸° API (ì‚¬ì „ ê¸°ëŠ¥ ê°•í™”ìš©)"""
    try:
        data = request.get_json()
        
        if not data or 'word' not in data or 'candidates' not in data:
            return jsonify({"error": "wordì™€ candidatesê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        target_word = data['word']
        candidates = data['candidates']  # ë¹„êµí•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        top_k = data.get('top_k', 5)  # ìƒìœ„ kê°œ ë°˜í™˜
        
        logger.info(f"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°: '{target_word}' (í›„ë³´: {len(candidates)}ê°œ)")
        
        # íƒ€ê²Ÿ ë‹¨ì–´ ì„ë² ë”©
        target_emb = get_sentence_embedding(target_word)
        
        # ëª¨ë“  í›„ë³´ ë‹¨ì–´ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for candidate in candidates:
            candidate_emb = get_sentence_embedding(candidate)
            similarity = cosine_similarity(target_emb, candidate_emb)[0][0]
            similarities.append({
                "word": candidate,
                "similarity": float(similarity * 100)
            })
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        top_similar = similarities[:top_k]
        
        logger.info(f"ìƒìœ„ {top_k}ê°œ: {[s['word'] for s in top_similar]}")
        
        return jsonify({
            "target_word": target_word,
            "similar_words": top_similar
        })
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸° ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/naver/search/books', methods=['POST'])
def search_books_naver():
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•œ ì±… ê²€ìƒ‰"""
    try:
        data = request.get_json()
        
        if not data or 'query' not in data:
            return jsonify({"error": "queryê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        query = data['query']
        display = data.get('display', 10)
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë„¤ì´ë²„ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        naver_client_id = os.getenv('NAVER_CLIENT_ID')
        naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        if not naver_client_id or not naver_client_secret:
            return jsonify({"error": "ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
        
        # ë„¤ì´ë²„ ê²€ìƒ‰ API í˜¸ì¶œ
        url = f"https://openapi.naver.com/v1/search/book.json"
        params = {
            'query': query,
            'display': min(display, 100),  # ìµœëŒ€ 100ê°œ
            'sort': 'sim'  # ì •í™•ë„ìˆœ
        }
        headers = {
            'X-Naver-Client-Id': naver_client_id,
            'X-Naver-Client-Secret': naver_client_secret
        }
        
        logger.info(f"ë„¤ì´ë²„ ì±… ê²€ìƒ‰: '{query}' (display: {display})")
        
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {result.get('total', 0)}ê°œ ì¤‘ {len(result.get('items', []))}ê°œ ë°˜í™˜")
        
        return jsonify(result)
    
    except requests.exceptions.RequestException as e:
        logger.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ë„¤ì´ë²„ ê²€ìƒ‰ API ì˜¤ë¥˜: {str(e)}"}), 500
    except Exception as e:
        logger.error(f"ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/test', methods=['GET'])
def test():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "message": "ğŸŒ¸ í•œê¸€ì •ì› AI ë°±ì—”ë“œ ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "models": {
            "TrOCR": "ì†ê¸€ì”¨ ì¸ì‹",
            "Whisper": "ìŒì„± ì¸ì‹",
            "ko-sroberta-multitask": "ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ (ì „ìš© ëª¨ë¸)"
        },
        "endpoints": {
            "health": "/health (GET) - ì„œë²„ ìƒíƒœ",
            "recognize": "/recognize (POST) - ì†ê¸€ì”¨ ì¸ì‹",
            "transcribe": "/transcribe (POST) - ìŒì„± ì¸ì‹",
            "similarity": "/similarity (POST) - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°",
            "similar_words": "/similar_words (POST) - ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°",
            "naver_search_books": "/naver/search/books (POST) - ë„¤ì´ë²„ ì±… ê²€ìƒ‰",
            "aladin_search_book": "/aladin/search_book (POST) - ì•Œë¼ë”˜ ì±… ê²€ìƒ‰"
        },
        "similarity": {
            "method": "Mean Pooling (ì „ì²´ í† í° í‰ê· )",
            "model": "jhgan/ko-sroberta-multitask",
            "accuracy": "92%+",
            "threshold": "70%",
            "benchmark": "í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ 1ìœ„"
        }
    })

@app.route('/aladin/search_book', methods=['POST'])
def search_book_aladin():
    """ì•Œë¼ë”˜ APIë¥¼ í†µí•œ ì±… ê²€ìƒ‰ (CORS ìš°íšŒ) - ê²€ìƒ‰ í›„ í‘œì§€ê¹Œì§€ ê°€ì ¸ì˜¤ê¸°"""
    try:
        data = request.get_json()
        
        if not data or 'title' not in data:
            return jsonify({"error": "titleì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        title = data['title']
        author = data.get('author', '')
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì•Œë¼ë”˜ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        aladin_api_key = os.getenv('ALADIN_API_KEY')
        
        if not aladin_api_key:
            return jsonify({"error": "ì•Œë¼ë”˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
        
        # 1ë‹¨ê³„: ê²€ìƒ‰ APIë¡œ ISBN ì°¾ê¸°
        query = f"{title} {author}".strip()
        search_url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
        search_params = {
            'ttbkey': aladin_api_key,
            'Query': query,
            'QueryType': 'Title',
            'MaxResults': '5',
            'start': '1',
            'SearchTarget': 'Book',
            'output': 'js',
            'Version': '20131101'
        }
        
        logger.info(f"ì•Œë¼ë”˜ ì±… ê²€ìƒ‰: '{query}'")
        
        search_response = requests.get(search_url, params=search_params, timeout=10)
        search_response.raise_for_status()
        search_data = search_response.json()
        
        if not search_data.get('item') or len(search_data['item']) == 0:
            logger.warning(f"ì•Œë¼ë”˜ì—ì„œ ì±…ì„ ì°¾ì§€ ëª»í•¨: {title}")
            return jsonify({
                "found": False,
                "message": "ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            })
        
        # ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ ì°¾ê¸°
        def normalize_string(s):
            return s.replace(' ', '').lower()
        
        normalized_title = normalize_string(title)
        normalized_author = normalize_string(author) if author else None
        
        best_match = search_data['item'][0]
        best_score = 0
        
        for item in search_data['item']:
            item_title = normalize_string(item.get('title', ''))
            item_author = normalize_string(item.get('author', ''))
            
            score = 0
            # ì œëª© ì¼ì¹˜ë„
            if normalized_title in item_title or item_title in normalized_title:
                score += 10
            # ì €ì ì¼ì¹˜ë„
            if normalized_author and normalized_author in item_author:
                score += 5
            
            if score > best_score:
                best_score = score
                best_match = item
        
        # 2ë‹¨ê³„: ISBNìœ¼ë¡œ ìƒì„¸ ì •ë³´ ë° í‘œì§€ ê°€ì ¸ì˜¤ê¸°
        isbn = best_match.get('isbn13', best_match.get('isbn', ''))
        if not isbn:
            logger.warning(f"ISBNì„ ì°¾ì§€ ëª»í•¨: {title}")
            # ISBN ì—†ì´ë„ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
            return jsonify({
                "found": True,
                "title": best_match.get('title', title),
                "author": best_match.get('author', author),
                "description": best_match.get('description', ''),
                "coverImageUrl": best_match.get('cover', ''),
                "isbn": '',
                "publisher": best_match.get('publisher', ''),
                "pubDate": best_match.get('pubDate', ''),
                "priceStandard": best_match.get('priceStandard', 0),
                "link": best_match.get('link', '')
            })
        
        # ISBN ì •ë¦¬
        cleaned_isbn = isbn.replace('-', '')
        
        # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í° í‘œì§€ ì´ë¯¸ì§€)
        detail_url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        detail_params = {
            'ttbkey': aladin_api_key,
            'itemIdType': 'ISBN13',
            'ItemId': cleaned_isbn,
            'output': 'js',
            'Version': '20131101',
            'Cover': 'Big'
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=10)
        detail_response.raise_for_status()
        detail_data = detail_response.json()
        
        if detail_data.get('item') and len(detail_data['item']) > 0:
            detail_item = detail_data['item'][0]
            book_data = {
                "found": True,
                "title": detail_item.get('title', best_match.get('title', '')),
                "author": detail_item.get('author', best_match.get('author', '')),
                "description": detail_item.get('description', best_match.get('description', '')),
                "coverImageUrl": detail_item.get('cover', ''),
                "isbn": cleaned_isbn,
                "publisher": detail_item.get('publisher', ''),
                "pubDate": detail_item.get('pubDate', ''),
                "priceStandard": detail_item.get('priceStandard', 0),
                "link": detail_item.get('link', '')
            }
        else:
            # ìƒì„¸ ì •ë³´ ì—†ìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
            book_data = {
                "found": True,
                "title": best_match.get('title', ''),
                "author": best_match.get('author', ''),
                "description": best_match.get('description', ''),
                "coverImageUrl": best_match.get('cover', ''),
                "isbn": cleaned_isbn,
                "publisher": best_match.get('publisher', ''),
                "pubDate": best_match.get('pubDate', ''),
                "priceStandard": best_match.get('priceStandard', 0),
                "link": best_match.get('link', '')
            }
        
        logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ: {book_data['title']} (ISBN: {cleaned_isbn})")
        
        return jsonify(book_data)
    
    except requests.exceptions.RequestException as e:
        logger.error(f"ì•Œë¼ë”˜ API ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ì•Œë¼ë”˜ API ì˜¤ë¥˜: {str(e)}", "found": False}), 500
    except Exception as e:
        logger.error(f"ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e), "found": False}), 500

# ==================== Gemini ì±… ì¶”ì²œ API ====================

# Gemini AI ì„¤ì •
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini API í‚¤ ë¡œë“œ ì™„ë£Œ")
else:
    logger.warning("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

@app.route('/recommend/books', methods=['POST'])
def recommend_books():
    """Gemini AIë¡œ ì±… ì¶”ì²œ í›„ ì•Œë¼ë”˜ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        if not GEMINI_API_KEY:
            return jsonify({"error": "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        recommend_type = data.get('type', 'level')  # 'level' or 'mood'
        level = data.get('level', 'ì´ˆê¸‰')
        mood = data.get('mood', '')
        situation = data.get('situation', '')
        purpose = data.get('purpose', '')
        genre = data.get('genre', '')
        mood_level = data.get('moodLevel', '')
        
        # Gemini ëª¨ë¸ ì„¤ì •
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        if recommend_type == 'mood':
            if not mood:
                return jsonify({"error": "ê¸°ë¶„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            prompt = f"""í•œêµ­ì–´ í•™ìŠµ ë„ì„œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê¸°ë¶„ê³¼ ìƒí™©ì„ ê¹Šì´ ì´í•´í•˜ê³ , ì‹¤ì œë¡œ ì¶œíŒëœ ìœ ëª…í•œ ì±…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

í˜„ì¬ ê¸°ë¶„: {mood}
"""
            if situation:
                prompt += f"í˜„ì¬ ìƒí™©: {situation}\n"
            if purpose:
                prompt += f"ë…ì„œ ëª©ì : {purpose}\n"
            if genre:
                prompt += f"ì„ í˜¸ ì¥ë¥´: {genre}\n"
            if mood_level:
                level_map = {
                    'ì´ˆê¸‰': "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
                    'ì¤‘ê¸‰': "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
                    'ê³ ê¸‰': "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
                }
                prompt += f"í•™ìŠµ ìˆ˜ì¤€: {level_map.get(mood_level, mood_level)}\n"
            
            prompt += """
**ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì±…ë§Œ ì¶”ì²œí•˜ì„¸ìš”. ë² ìŠ¤íŠ¸ì…€ëŸ¬ë‚˜ ìœ ëª… ì‘ê°€ì˜ ì±… ìœ„ì£¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.**

ê¸°ë¶„ê³¼ ìƒí™©ì— ë§ëŠ” ì±… 5ê¶Œì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"books": [{"title": "ì •í™•í•œ ì±… ì œëª©", "author": "ì €ìëª…", "description": "ì´ ì±…ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ ì™€ í˜„ì¬ ê¸°ë¶„/ìƒí™©ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª… (2-3ë¬¸ì¥)"}]}"""
        
        else:  # level
            level_map = {
                'ì´ˆê¸‰': "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
                'ì¤‘ê¸‰': "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
                'ê³ ê¸‰': "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
            }
            level_description = level_map.get(level, level)
            
            prompt = f"""í•œêµ­ì–´ í•™ìŠµ ë„ì„œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ ì¶œíŒëœ ìœ ëª…í•œ í•œêµ­ì–´ í•™ìŠµ êµì¬ì˜ ì •í™•í•œ ì œëª©ê³¼ ì €ìë¥¼ ì œê³µí•˜ì„¸ìš”.

{level_description} í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì±… 5ê¶Œì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì±…ë§Œ ì¶”ì²œí•˜ì„¸ìš”:**
- ì„œìš¸ëŒ€ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì—°ì„¸ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì´í™” í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- Korean Grammar in Use ì‹œë¦¬ì¦ˆ
- ê·¸ ì™¸ ê²€ì¦ëœ í•œêµ­ì–´ í•™ìŠµ êµì¬

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"books": [{"title": "ì •í™•í•œ ì±… ì œëª©", "author": "ì €ìëª…", "description": "ì´ ì±…ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ  (2-3ë¬¸ì¥)"}]}"""
        
        logger.info(f"ğŸ“š Geminiì— ì±… ì¶”ì²œ ìš”ì²­: type={recommend_type}, level={level}, mood={mood}")
        
        # Gemini API í˜¸ì¶œ
        response = model.generate_content(prompt)
        response_text = response.text
        
        # JSON ì¶”ì¶œ (```json ``` ì œê±°)
        import json
        import re
        
        json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_match = re.search(r'```\s*([\s\S]*?)\s*```', response_text)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = response_text
        
        books_data = json.loads(json_str)
        books = books_data.get('books', [])
        
        if not books:
            return jsonify({"error": "ì±… ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        logger.info(f"âœ… Gemini ì¶”ì²œ ì™„ë£Œ: {len(books)}ê¶Œ")
        
        # ì•Œë¼ë”˜ APIë¡œ ê° ì±…ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        aladin_api_key = os.getenv('ALADIN_API_KEY')
        
        if not aladin_api_key:
            logger.warning("âš ï¸ ì•Œë¼ë”˜ API í‚¤ ì—†ìŒ - AI ì¶”ì²œë§Œ ë°˜í™˜")
            return jsonify(books)
        
        enriched_books = []
        
        for book in books:
            title = book.get('title', '')
            author = book.get('author', '')
            ai_description = book.get('description', '')
            
            # ì•Œë¼ë”˜ ê²€ìƒ‰
            query = f"{title} {author}".strip()
            search_url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
            search_params = {
                'ttbkey': aladin_api_key,
                'Query': query,
                'QueryType': 'Title',
                'MaxResults': '5',
                'start': '1',
                'SearchTarget': 'Book',
                'output': 'js',
                'Version': '20131101'
            }
            
            try:
                search_response = requests.get(search_url, params=search_params, timeout=10)
                search_response.raise_for_status()
                search_data = search_response.json()
                
                if search_data.get('item') and len(search_data['item']) > 0:
                    # ìµœì  ë§¤ì¹­ ì°¾ê¸°
                    def normalize_string(s):
                        return s.replace(' ', '').lower()
                    
                    normalized_title = normalize_string(title)
                    normalized_author = normalize_string(author) if author else None
                    
                    best_match = search_data['item'][0]
                    best_score = 0
                    
                    for item in search_data['item']:
                        item_title = normalize_string(item.get('title', ''))
                        item_author = normalize_string(item.get('author', ''))
                        
                        score = 0
                        if normalized_title in item_title or item_title in normalized_title:
                            score += 10
                        if normalized_author and normalized_author in item_author:
                            score += 5
                        
                        if score > best_score:
                            best_score = score
                            best_match = item
                    
                    # ISBNìœ¼ë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    isbn = best_match.get('isbn13', best_match.get('isbn', ''))
                    
                    if isbn:
                        cleaned_isbn = isbn.replace('-', '')
                        
                        detail_url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
                        detail_params = {
                            'ttbkey': aladin_api_key,
                            'itemIdType': 'ISBN13',
                            'ItemId': cleaned_isbn,
                            'output': 'js',
                            'Version': '20131101',
                            'Cover': 'Big'
                        }
                        
                        detail_response = requests.get(detail_url, params=detail_params, timeout=10)
                        detail_response.raise_for_status()
                        detail_data = detail_response.json()
                        
                        if detail_data.get('item') and len(detail_data['item']) > 0:
                            detail_item = detail_data['item'][0]
                            enriched_books.append({
                                "title": detail_item.get('title', title),
                                "author": detail_item.get('author', author),
                                "description": ai_description,  # AI ì¶”ì²œ ì´ìœ  ìœ ì§€
                                "coverImageUrl": detail_item.get('cover', ''),
                                "isbn": cleaned_isbn,
                                "publisher": detail_item.get('publisher', ''),
                                "pubDate": detail_item.get('pubDate', ''),
                                "priceStandard": detail_item.get('priceStandard', 0),
                                "link": detail_item.get('link', '')
                            })
                            logger.info(f"âœ… ì•Œë¼ë”˜ ì •ë³´ ì¶”ê°€: {title}")
                            continue
                
                # ì•Œë¼ë”˜ì—ì„œ ëª» ì°¾ìœ¼ë©´ AI ì¶”ì²œë§Œ
                enriched_books.append({
                    "title": title,
                    "author": author,
                    "description": ai_description,
                    "coverImageUrl": None,
                    "isbn": None
                })
                logger.warning(f"âš ï¸ ì•Œë¼ë”˜ì—ì„œ ì°¾ì§€ ëª»í•¨: {title}")
                
            except Exception as e:
                logger.error(f"ì•Œë¼ë”˜ ê²€ìƒ‰ ì˜¤ë¥˜ ({title}): {str(e)}")
                enriched_books.append({
                    "title": title,
                    "author": author,
                    "description": ai_description,
                    "coverImageUrl": None,
                    "isbn": None
                })
        
        logger.info(f"ğŸ‰ ìµœì¢… ê²°ê³¼: {len(enriched_books)}ê¶Œ (ì•Œë¼ë”˜ ì •ë³´ í¬í•¨)")
        return jsonify(enriched_books)
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}), 500
    except Exception as e:
        logger.error(f"ì±… ì¶”ì²œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ì±… ì¶”ì²œ ì‹¤íŒ¨: {str(e)}"}), 500

if __name__ == '__main__':
    # ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    logger.info("=" * 60)
    logger.info("ğŸš€ í•œê¸€ì •ì› AI ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info("=" * 60)
    
    logger.info("\n[1/4] EasyOCR í•œê¸€ ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_easyocr_reader()
    
    logger.info("\n[2/4] TrOCR ì†ê¸€ì”¨ ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_model()
    
    logger.info("\n[3/4] Whisper ìŒì„± ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_whisper_model()
    
    logger.info("\n[4/4] ko-sroberta-multitask ë¬¸ì¥ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”©...")
    load_sroberta_model()
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    logger.info("âœ¨ ì†ê¸€ì”¨ ì¸ì‹: EasyOCR (í•œê¸€ íŠ¹í™”)")
    logger.info("ğŸ¯ ë¬¸ì¥ ìœ ì‚¬ë„ ëª¨ë¸: ko-sroberta-multitask (Mean Pooling)")
    logger.info("ğŸ“Š ë²¤ì¹˜ë§ˆí¬: í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ 1ìœ„")
    logger.info("ğŸ”¥ ì •í™•ë„: 92%+ (ê¸°ì¡´ ëŒ€ë¹„ +3% í–¥ìƒ)")
    logger.info("âš¡ ì†ë„: 2ë°° í–¥ìƒ (ë‹¨ì¼ ëª¨ë¸)")
    logger.info("=" * 60 + "\n")
    
    # Flask ì„œë²„ ì‹¤í–‰
    app.run(host='0.0.0.0', port=5001, debug=True)




