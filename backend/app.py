from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoModel, AutoTokenizer
import io
import base64
import logging
import whisper
import tempfile
import os
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import requests
from dotenv import load_dotenv
import easyocr
import google.generativeai as genai

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# CORS ì„¤ì • - ëª¨ë“  ì¶œì²˜ í—ˆìš©
# flask-corsê°€ ìë™ìœ¼ë¡œ í—¤ë”ë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ìˆ˜ë™ ì¶”ê°€ëŠ” ì œê±°
CORS(app, resources={
    r"/*": {
        "origins": "*",
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization", "X-Requested-With"],
        "supports_credentials": False
    }
})

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ ì €ì¥ (í•œ ë²ˆë§Œ ë¡œë“œ)
processor = None
model = None
whisper_model = None
sroberta_model = None
sroberta_tokenizer = None
easyocr_reader = None

def load_model():
    """TrOCR ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global processor, model
    
    if processor is None or model is None:
        logger.info("TrOCR ëª¨ë¸ ë¡œë”© ì¤‘...")
        processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')
        model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if torch.cuda.is_available():
            model = model.to('cuda')
            logger.info("GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logger.info("CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        logger.info("TrOCR ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return processor, model

def load_easyocr_reader():
    """EasyOCR Reader ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global easyocr_reader
    
    if easyocr_reader is None:
        logger.info("EasyOCR í•œê¸€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        # í•œê¸€ê³¼ ì˜ì–´ë¥¼ ì§€ì›í•˜ëŠ” Reader ìƒì„±
        easyocr_reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())
        logger.info("EasyOCR ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return easyocr_reader

def load_whisper_model():
    """Whisper ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global whisper_model
    
    if whisper_model is None:
        logger.info("Whisper Small ëª¨ë¸ ë¡œë”© ì¤‘...")
        whisper_model = whisper.load_model("small")  # small, medium, large ì¤‘ ì„ íƒ
        logger.info("Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return whisper_model

def load_sroberta_model():
    """ko-sroberta-multitask ëª¨ë¸ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)"""
    global sroberta_model, sroberta_tokenizer
    
    if sroberta_model is None or sroberta_tokenizer is None:
        logger.info("ko-sroberta-multitask ëª¨ë¸ ë¡œë”© ì¤‘...")
        sroberta_tokenizer = AutoTokenizer.from_pretrained("jhgan/ko-sroberta-multitask")
        sroberta_model = AutoModel.from_pretrained("jhgan/ko-sroberta-multitask")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
        if torch.cuda.is_available():
            sroberta_model = sroberta_model.to('cuda')
            logger.info("ko-sroberta: GPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            logger.info("ko-sroberta: CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        sroberta_model.eval()  # í‰ê°€ ëª¨ë“œ
        logger.info("ko-sroberta-multitask ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    return sroberta_model, sroberta_tokenizer

def mean_pooling(model_output, attention_mask):
    """Mean Pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê·  ê³„ì‚°"""
    token_embeddings = model_output[0]  # ëª¨ë“  í† í° ì„ë² ë”©
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def get_sentence_embedding(text):
    """
    ë¬¸ì¥ ì„ë² ë”© ìƒì„± (ko-sroberta-multitask + Mean Pooling)
    
    Args:
        text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
    
    Returns:
        numpy array: ë¬¸ì¥ ì„ë² ë”©
    """
    logger.info("ğŸ¯ ko-sroberta-multitaskë¡œ ì„ë² ë”© ìƒì„±")
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_sroberta_model()
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°)
    if isinstance(text, str):
        text = [text]
    
    # í† í¬ë‚˜ì´ì§•
    encoded_input = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')
    
    # GPU ì‚¬ìš© ì‹œ í…ì„œë„ GPUë¡œ
    if torch.cuda.is_available():
        encoded_input = {k: v.to('cuda') for k, v in encoded_input.items()}
    
    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        model_output = model(**encoded_input)
    
    # Mean Pooling
    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
    
    # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
    embeddings = sentence_embeddings.cpu().numpy()
    
    logger.info("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    return embeddings

@app.route('/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "ok",
        "models": {
            "trocr": "microsoft/trocr-base-handwritten",
            "whisper": "openai/whisper-small",
            "sroberta": "jhgan/ko-sroberta-multitask"
        },
        "similarity_model": "ko-sroberta-multitask (Mean Pooling)",
        "accuracy": "92%+",
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    })

@app.route('/recognize', methods=['POST'])
def recognize_handwriting():
    """ì†ê¸€ì”¨ ì¸ì‹ API - EasyOCR ì‚¬ìš© (ì „ì²˜ë¦¬ ê°•í™”)"""
    try:
        # ìš”ì²­ ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # Base64 ì´ë¯¸ì§€ ë””ì½”ë”©
        image_base64 = data['image']
        
        # data:image/png;base64, ë¶€ë¶„ ì œê±°
        if ',' in image_base64:
            image_base64 = image_base64.split(',')[1]
        
        image_bytes = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        logger.info(f"ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°: {image.size}")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ê°„ë‹¨í•˜ê²Œ)
        # 1. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        image_gray = image.convert('L')
        
        # 2. ì•½í•œ ëŒ€ë¹„ ì¦ê°€
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Contrast(image_gray)
        image_enhanced = enhancer.enhance(1.3)  # ëŒ€ë¹„ ì•½í•˜ê²Œ ì¦ê°€
        
        # 3. numpy ë°°ì—´ë¡œ ë³€í™˜
        image_np = np.array(image_enhanced)
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ - í¬ê¸°: {image_np.shape}")
        
        # EasyOCR Reader ë¡œë“œ
        reader = load_easyocr_reader()
        
        # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¸ì‹ (íŒŒë¼ë¯¸í„° ìµœì í™”)
        results = reader.readtext(
            image_np,
            detail=1,
            paragraph=False,
            min_size=5,  # ìµœì†Œ í…ìŠ¤íŠ¸ í¬ê¸° ë” ë‚®ì¶¤
            text_threshold=0.1,  # í…ìŠ¤íŠ¸ ê°ì§€ ì„ê³„ê°’ ë” ë‚®ì¶¤
            low_text=0.1,  # ë‚®ì€ í…ìŠ¤íŠ¸ ì ìˆ˜ ë” í—ˆìš©
            link_threshold=0.1,
            canvas_size=2560,
            mag_ratio=1.5,
            contrast_ths=0.1,  # ëŒ€ë¹„ ì„ê³„ê°’ ë‚®ì¶¤
            adjust_contrast=0.5  # ëŒ€ë¹„ ì¡°ì • ì•½í•˜ê²Œ
        )
        
        # ê²°ê³¼ ì²˜ë¦¬
        logger.info(f"EasyOCR ê²°ê³¼ ê°œìˆ˜: {len(results)}")
        
        if results:
            # ëª¨ë“  ê²°ê³¼ ë¡œê¹…
            for idx, (bbox, text, conf) in enumerate(results):
                logger.info(f"  ê²°ê³¼ {idx+1}: '{text}' (ì‹ ë¢°ë„: {conf:.2f})")
            
            # ì‹ ë¢°ë„ê°€ ê°€ì¥ ë†’ì€ ê²°ê³¼ ì„ íƒ
            best_result = max(results, key=lambda x: x[2])
            recognized_text = best_result[1]
            confidence = best_result[2]
            
            # í•œê¸€ë§Œ ì¶”ì¶œ (ìˆ«ì/ì˜ì–´ ì œê±°)
            import re
            korean_only = re.sub(r'[^ê°€-í£ã„±-ã…ã…-ã…£]', '', recognized_text)
            
            if korean_only:
                logger.info(f"âœ… ìµœì¢… ì¸ì‹: {korean_only} (ì‹ ë¢°ë„: {confidence:.2f})")
                return jsonify({
                    "text": korean_only,
                    "confidence": float(confidence),
                    "method": "easyocr",
                    "raw_text": recognized_text
                })
            else:
                logger.warning(f"âš ï¸ í•œê¸€ì´ ì—†ìŒ: {recognized_text}")
                return jsonify({
                    "text": recognized_text,  # í•œê¸€ì´ ì—†ì–´ë„ ì›ë³¸ ë°˜í™˜
                    "confidence": float(confidence),
                    "method": "easyocr"
                })
        else:
            logger.warning("âŒ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return jsonify({
                "text": "",
                "confidence": 0.0,
                "method": "easyocr",
                "message": "í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            })
    
    except Exception as e:
        logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/recognize-mcp', methods=['POST'])
def recognize_handwriting_mcp():
    """MCPë¥¼ ì‚¬ìš©í•œ ì†ê¸€ì”¨ ì¸ì‹ API"""
    try:
        import subprocess
        import json as json_lib
        import os
        
        # ìš”ì²­ ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        if not data or 'image' not in data:
            return jsonify({"error": "ì´ë¯¸ì§€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        # Base64 ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
        image_base64 = data['image']
        if ',' in image_base64:
            image_base64 = image_base64.split(',')[1]
        
        language = data.get('language', 'ko')
        
        logger.info("MCPë¥¼ ì‚¬ìš©í•œ ì†ê¸€ì”¨ ì¸ì‹ ì‹œì‘...")
        
        # Node.js MCP í”„ë¡ì‹œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        try:
            # í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            current_dir = os.path.dirname(os.path.abspath(__file__))
            mcp_proxy_path = os.path.join(current_dir, 'mcp_proxy.js')
            
            # Node.jsë¥¼ í†µí•´ MCP í”„ë¡ì‹œ ì‹¤í–‰
            result = subprocess.run(
                ['node', mcp_proxy_path, image_base64, language],
                capture_output=True,
                text=True,
                timeout=30,
                cwd=current_dir
            )
            
            if result.returncode != 0:
                error_output = result.stderr or result.stdout
                logger.warning(f"MCP í˜¸ì¶œ ì‹¤íŒ¨: {error_output}")
                # stderrì— ë¡œê·¸ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                try:
                    error_data = json_lib.loads(error_output)
                    raise Exception(error_data.get('error', 'MCP í˜¸ì¶œ ì‹¤íŒ¨'))
                except json_lib.JSONDecodeError:
                    raise Exception(f"MCP í˜¸ì¶œ ì‹¤íŒ¨: {error_output}")
            
            # ê²°ê³¼ íŒŒì‹± (stdoutì—ì„œ JSON ì½ê¸°)
            output_lines = result.stdout.strip().split('\n')
            # ë§ˆì§€ë§‰ JSON ë¼ì¸ ì°¾ê¸° (ë¡œê·¸ê°€ ì„ì—¬ ìˆì„ ìˆ˜ ìˆìŒ)
            json_output = None
            for line in reversed(output_lines):
                line = line.strip()
                if line.startswith('{') and line.endswith('}'):
                    try:
                        json_output = json_lib.loads(line)
                        break
                    except json_lib.JSONDecodeError:
                        continue
            
            if not json_output:
                # ì „ì²´ stdoutì„ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
                try:
                    json_output = json_lib.loads(result.stdout)
                except json_lib.JSONDecodeError:
                    raise Exception(f"MCP ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {result.stdout}")
            
            recognized_text = json_output.get('text', json_output.get('result', ''))
            
            if not recognized_text:
                logger.warning(f"MCP ê²°ê³¼ì— textê°€ ì—†ìŒ: {json_output}")
                raise Exception("MCPì—ì„œ ì¸ì‹ ê²°ê³¼ë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"MCP ì¸ì‹ ê²°ê³¼: {recognized_text}")
            
            return jsonify({
                "text": recognized_text,
                "method": "mcp"
            })
            
        except subprocess.TimeoutExpired:
            logger.error("MCP í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ")
            raise Exception("MCP í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)")
        except json_lib.JSONDecodeError as e:
            logger.error(f"MCP ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}, stdout: {result.stdout if 'result' in locals() else 'N/A'}")
            raise Exception(f"MCP ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"MCP í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
            raise
    
    except Exception as e:
        logger.error(f"MCP ì†ê¸€ì”¨ ì¸ì‹ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/transcribe', methods=['POST'])
def transcribe_audio():
    """Whisper ìŒì„± ì¸ì‹ API"""
    try:
        # íŒŒì¼ ë°›ê¸°
        if 'file' not in request.files:
            return jsonify({"error": "ì˜¤ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        audio_file = request.files['file']
        
        if audio_file.filename == '':
            return jsonify({"error": "íŒŒì¼ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 400
        
        logger.info(f"ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜ì‹ : {audio_file.filename}, íƒ€ì…: {audio_file.content_type}")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.webm') as temp_file:
            audio_file.save(temp_file.name)
            temp_path = temp_file.name
        
        try:
            # Whisper ëª¨ë¸ ë¡œë“œ
            model = load_whisper_model()
            
            # ìŒì„± ì¸ì‹ (í•œêµ­ì–´ë¡œ ëª…ì‹œ)
            logger.info("Whisperë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
            result = model.transcribe(temp_path, language='ko', fp16=False)
            
            transcription = result['text'].strip()
            logger.info(f"ì¸ì‹ ê²°ê³¼: {transcription}")
            
            return jsonify({
                "text": transcription,
                "language": "ko"
            })
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.unlink(temp_path)
    
    except Exception as e:
        logger.error(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/similarity', methods=['POST'])
def calculate_similarity():
    """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° API (ì£¼ê´€ì‹ ë‹µë³€ ì±„ì ìš©)"""
    try:
        data = request.get_json()
        
        if not data or 'text1' not in data or 'text2' not in data:
            return jsonify({"error": "text1ê³¼ text2ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        text1 = data['text1']
        text2 = data['text2']
        
        logger.info(f"ìœ ì‚¬ë„ ê³„ì‚°: '{text1}' vs '{text2}'")
        
        # ì„ë² ë”© ìƒì„±
        emb1 = get_sentence_embedding(text1)
        emb2 = get_sentence_embedding(text2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity(emb1, emb2)[0][0]
        similarity_percent = float(similarity * 100)
        
        logger.info(f"ìœ ì‚¬ë„: {similarity_percent:.2f}%")
        
        return jsonify({
            "similarity": similarity_percent,
            "text1": text1,
            "text2": text2,
            "is_similar": similarity_percent >= 70  # 70% ì´ìƒì´ë©´ ìœ ì‚¬í•˜ë‹¤ê³  íŒë‹¨
        })
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/similar_words', methods=['POST'])
def find_similar_words():
    """ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸° API (ì‚¬ì „ ê¸°ëŠ¥ ê°•í™”ìš©)"""
    try:
        data = request.get_json()
        
        if not data or 'word' not in data or 'candidates' not in data:
            return jsonify({"error": "wordì™€ candidatesê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        target_word = data['word']
        candidates = data['candidates']  # ë¹„êµí•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
        top_k = data.get('top_k', 5)  # ìƒìœ„ kê°œ ë°˜í™˜
        
        logger.info(f"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°: '{target_word}' (í›„ë³´: {len(candidates)}ê°œ)")
        
        # íƒ€ê²Ÿ ë‹¨ì–´ ì„ë² ë”©
        target_emb = get_sentence_embedding(target_word)
        
        # ëª¨ë“  í›„ë³´ ë‹¨ì–´ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for candidate in candidates:
            candidate_emb = get_sentence_embedding(candidate)
            similarity = cosine_similarity(target_emb, candidate_emb)[0][0]
            similarities.append({
                "word": candidate,
                "similarity": float(similarity * 100)
            })
        
        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ìƒìœ„ kê°œ ë°˜í™˜
        top_similar = similarities[:top_k]
        
        logger.info(f"ìƒìœ„ {top_k}ê°œ: {[s['word'] for s in top_similar]}")
        
        return jsonify({
            "target_word": target_word,
            "similar_words": top_similar
        })
    
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸° ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/naver/search/books', methods=['POST'])
def search_books_naver():
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ í†µí•œ ì±… ê²€ìƒ‰"""
    try:
        data = request.get_json()
        
        if not data or 'query' not in data:
            return jsonify({"error": "queryê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        query = data['query']
        display = data.get('display', 10)
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë„¤ì´ë²„ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        naver_client_id = os.getenv('NAVER_CLIENT_ID')
        naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        if not naver_client_id or not naver_client_secret:
            return jsonify({"error": "ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
        
        # ë„¤ì´ë²„ ê²€ìƒ‰ API í˜¸ì¶œ
        url = f"https://openapi.naver.com/v1/search/book.json"
        params = {
            'query': query,
            'display': min(display, 100),  # ìµœëŒ€ 100ê°œ
            'sort': 'sim'  # ì •í™•ë„ìˆœ
        }
        headers = {
            'X-Naver-Client-Id': naver_client_id,
            'X-Naver-Client-Secret': naver_client_secret
        }
        
        logger.info(f"ë„¤ì´ë²„ ì±… ê²€ìƒ‰: '{query}' (display: {display})")
        
        response = requests.get(url, params=params, headers=headers)
        response.raise_for_status()
        
        result = response.json()
        
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {result.get('total', 0)}ê°œ ì¤‘ {len(result.get('items', []))}ê°œ ë°˜í™˜")
        
        return jsonify(result)
    
    except requests.exceptions.RequestException as e:
        logger.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ë„¤ì´ë²„ ê²€ìƒ‰ API ì˜¤ë¥˜: {str(e)}"}), 500
    except Exception as e:
        logger.error(f"ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e)}), 500

@app.route('/test', methods=['GET'])
def test():
    """í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "message": "ğŸŒ¸ í•œê¸€ì •ì› AI ë°±ì—”ë“œ ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!",
        "models": {
            "TrOCR": "ì†ê¸€ì”¨ ì¸ì‹",
            "Whisper": "ìŒì„± ì¸ì‹",
            "ko-sroberta-multitask": "ë¬¸ì¥ ìœ ì‚¬ë„ ë¶„ì„ (ì „ìš© ëª¨ë¸)"
        },
        "endpoints": {
            "health": "/health (GET) - ì„œë²„ ìƒíƒœ",
            "recognize": "/recognize (POST) - ì†ê¸€ì”¨ ì¸ì‹",
            "transcribe": "/transcribe (POST) - ìŒì„± ì¸ì‹",
            "similarity": "/similarity (POST) - í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°",
            "similar_words": "/similar_words (POST) - ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°",
            "naver_search_books": "/naver/search/books (POST) - ë„¤ì´ë²„ ì±… ê²€ìƒ‰",
            "aladin_search_book": "/aladin/search_book (POST) - ì•Œë¼ë”˜ ì±… ê²€ìƒ‰"
        },
        "similarity": {
            "method": "Mean Pooling (ì „ì²´ í† í° í‰ê· )",
            "model": "jhgan/ko-sroberta-multitask",
            "accuracy": "92%+",
            "threshold": "70%",
            "benchmark": "í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ 1ìœ„"
        }
    })

@app.route('/aladin/search_book', methods=['POST'])
def search_book_aladin():
    """ì•Œë¼ë”˜ APIë¥¼ í†µí•œ ì±… ê²€ìƒ‰ (CORS ìš°íšŒ) - ê²€ìƒ‰ í›„ í‘œì§€ê¹Œì§€ ê°€ì ¸ì˜¤ê¸°"""
    try:
        data = request.get_json()
        
        if not data or 'title' not in data:
            return jsonify({"error": "titleì´ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        title = data['title']
        author = data.get('author', '')
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì•Œë¼ë”˜ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        aladin_api_key = os.getenv('ALADIN_API_KEY')
        
        if not aladin_api_key:
            return jsonify({"error": "ì•Œë¼ë”˜ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."}), 500
        
        # 1ë‹¨ê³„: ê²€ìƒ‰ APIë¡œ ISBN ì°¾ê¸°
        query = f"{title} {author}".strip()
        search_url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
        search_params = {
            'ttbkey': aladin_api_key,
            'Query': query,
            'QueryType': 'Title',
            'MaxResults': '5',
            'start': '1',
            'SearchTarget': 'Book',
            'output': 'js',
            'Version': '20131101'
        }
        
        logger.info(f"ì•Œë¼ë”˜ ì±… ê²€ìƒ‰: '{query}'")
        
        search_response = requests.get(search_url, params=search_params, timeout=10)
        search_response.raise_for_status()
        search_data = search_response.json()
        
        if not search_data.get('item') or len(search_data['item']) == 0:
            logger.warning(f"ì•Œë¼ë”˜ì—ì„œ ì±…ì„ ì°¾ì§€ ëª»í•¨: {title}")
            return jsonify({
                "found": False,
                "message": "ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            })
        
        # ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ ì°¾ê¸°
        def normalize_string(s):
            return s.replace(' ', '').lower()
        
        normalized_title = normalize_string(title)
        normalized_author = normalize_string(author) if author else None
        
        best_match = search_data['item'][0]
        best_score = 0
        
        for item in search_data['item']:
            item_title = normalize_string(item.get('title', ''))
            item_author = normalize_string(item.get('author', ''))
            
            score = 0
            # ì œëª© ì¼ì¹˜ë„
            if normalized_title in item_title or item_title in normalized_title:
                score += 10
            # ì €ì ì¼ì¹˜ë„
            if normalized_author and normalized_author in item_author:
                score += 5
            
            if score > best_score:
                best_score = score
                best_match = item
        
        # 2ë‹¨ê³„: ISBNìœ¼ë¡œ ìƒì„¸ ì •ë³´ ë° í‘œì§€ ê°€ì ¸ì˜¤ê¸°
        isbn = best_match.get('isbn13', best_match.get('isbn', ''))
        if not isbn:
            logger.warning(f"ISBNì„ ì°¾ì§€ ëª»í•¨: {title}")
            # ISBN ì—†ì´ë„ ê¸°ë³¸ ì •ë³´ ë°˜í™˜
            return jsonify({
                "found": True,
                "title": best_match.get('title', title),
                "author": best_match.get('author', author),
                "description": best_match.get('description', ''),
                "coverImageUrl": best_match.get('cover', ''),
                "isbn": '',
                "publisher": best_match.get('publisher', ''),
                "pubDate": best_match.get('pubDate', ''),
                "priceStandard": best_match.get('priceStandard', 0),
                "link": best_match.get('link', '')
            })
        
        # ISBN ì •ë¦¬
        cleaned_isbn = isbn.replace('-', '')
        
        # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í° í‘œì§€ ì´ë¯¸ì§€)
        detail_url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        detail_params = {
            'ttbkey': aladin_api_key,
            'itemIdType': 'ISBN13',
            'ItemId': cleaned_isbn,
            'output': 'js',
            'Version': '20131101',
            'Cover': 'Big'
        }
        
        detail_response = requests.get(detail_url, params=detail_params, timeout=10)
        detail_response.raise_for_status()
        detail_data = detail_response.json()
        
        if detail_data.get('item') and len(detail_data['item']) > 0:
            detail_item = detail_data['item'][0]
            book_data = {
                "found": True,
                "title": detail_item.get('title', best_match.get('title', '')),
                "author": detail_item.get('author', best_match.get('author', '')),
                "description": detail_item.get('description', best_match.get('description', '')),
                "coverImageUrl": detail_item.get('cover', ''),
                "isbn": cleaned_isbn,
                "publisher": detail_item.get('publisher', ''),
                "pubDate": detail_item.get('pubDate', ''),
                "priceStandard": detail_item.get('priceStandard', 0),
                "link": detail_item.get('link', '')
            }
        else:
            # ìƒì„¸ ì •ë³´ ì—†ìœ¼ë©´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
            book_data = {
                "found": True,
                "title": best_match.get('title', ''),
                "author": best_match.get('author', ''),
                "description": best_match.get('description', ''),
                "coverImageUrl": best_match.get('cover', ''),
                "isbn": cleaned_isbn,
                "publisher": best_match.get('publisher', ''),
                "pubDate": best_match.get('pubDate', ''),
                "priceStandard": best_match.get('priceStandard', 0),
                "link": best_match.get('link', '')
            }
        
        logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ: {book_data['title']} (ISBN: {cleaned_isbn})")
        
        return jsonify(book_data)
    
    except requests.exceptions.RequestException as e:
        logger.error(f"ì•Œë¼ë”˜ API ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ì•Œë¼ë”˜ API ì˜¤ë¥˜: {str(e)}", "found": False}), 500
    except Exception as e:
        logger.error(f"ì±… ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": str(e), "found": False}), 500

# ==================== Gemini ì±… ì¶”ì²œ API ====================

# Gemini AI ì„¤ì •
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    logger.info("âœ… Gemini API í‚¤ ë¡œë“œ ì™„ë£Œ")
else:
    logger.warning("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

@app.route('/recommend/books', methods=['POST'])
def recommend_books():
    """Gemini AIë¡œ ì±… ì¶”ì²œ í›„ ì•Œë¼ë”˜ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        data = request.get_json()
        
        if not data:
            logger.error("ìš”ì²­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return jsonify({"error": "ìš”ì²­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
        
        if not GEMINI_API_KEY:
            logger.error("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return jsonify({"error": "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 500
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        recommend_type = data.get('type', 'level')  # 'level' or 'mood'
        level = data.get('level', 'ì´ˆê¸‰')
        mood = data.get('mood', '')
        situation = data.get('situation', '')
        purpose = data.get('purpose', '')
        genre = data.get('genre', '')
        mood_level = data.get('moodLevel', '')
        
        # Gemini ëª¨ë¸ ì„¤ì • - 2nd Book Pharmacy í”„ë¡œì íŠ¸ ì°¸ê³ : gemini-2.5-flash ì‚¬ìš©
        model = None
        model_names = [
            'gemini-2.5-flash',      # ìµœì‹  ëª¨ë¸ (2nd Book Pharmacyì—ì„œ ì‚¬ìš©)
            'gemini-1.5-flash',      # ë¹ ë¥¸ ëª¨ë¸
            'gemini-1.5-pro',        # í”„ë¡œ ëª¨ë¸
            'gemini-pro',            # êµ¬ë²„ì „ (fallback)
        ]
        
        for model_name in model_names:
            try:
                model = genai.GenerativeModel(model_name)
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ ëª¨ë¸ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
                logger.info(f"âœ… Gemini ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                break
            except Exception as e:
                logger.warning(f"âš ï¸ {model_name} ì‹¤íŒ¨: {str(e)}")
                continue
        
        if model is None:
            logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ Gemini ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return jsonify({"error": "Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ì™€ ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”."}), 500
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        if recommend_type == 'mood':
            if not mood:
                return jsonify({"error": "ê¸°ë¶„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400
            
            prompt = f"""í•œêµ­ì–´ í•™ìŠµ ë„ì„œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê¸°ë¶„ê³¼ ìƒí™©ì„ ê¹Šì´ ì´í•´í•˜ê³ , ì‹¤ì œë¡œ ì¶œíŒëœ ìœ ëª…í•œ ì±…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

í˜„ì¬ ê¸°ë¶„: {mood}
"""
            if situation:
                prompt += f"í˜„ì¬ ìƒí™©: {situation}\n"
            if purpose:
                prompt += f"ë…ì„œ ëª©ì : {purpose}\n"
            if genre:
                prompt += f"ì„ í˜¸ ì¥ë¥´: {genre}\n"
            if mood_level:
                level_map = {
                    'ì´ˆê¸‰': "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
                    'ì¤‘ê¸‰': "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
                    'ê³ ê¸‰': "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
                }
                prompt += f"í•™ìŠµ ìˆ˜ì¤€: {level_map.get(mood_level, mood_level)}\n"
            
            prompt += """
**ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì±…ë§Œ ì¶”ì²œí•˜ì„¸ìš”. ë² ìŠ¤íŠ¸ì…€ëŸ¬ë‚˜ ìœ ëª… ì‘ê°€ì˜ ì±… ìœ„ì£¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.**

ê¸°ë¶„ê³¼ ìƒí™©ì— ë§ëŠ” ì±… 5ê¶Œì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. 
**ì¤‘ìš”**: ê° ì±…ì˜ ì •í™•í•œ ISBN-13ì„ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”. ISBNì´ ì—†ìœ¼ë©´ ì•Œë¼ë”˜ì—ì„œ ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"books": [{"title": "ì •í™•í•œ ì±… ì œëª© (ì¶œíŒì‚¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •í™•í•œ ì œëª©)", "author": "ì €ìëª…", "isbn13": "ISBN-13 ë²ˆí˜¸ (í•˜ì´í”ˆ í¬í•¨, ì˜ˆ: 978-89-1234-567-8)", "description": "ì´ ì±…ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ ì™€ í˜„ì¬ ê¸°ë¶„/ìƒí™©ì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ëŠ”ì§€ ì„¤ëª… (2-3ë¬¸ì¥)"}]}

**ISBN-13ì„ ëª¨ë¥´ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‘ë˜, ë°˜ë“œì‹œ ì •í™•í•œ ì±… ì œëª©ê³¼ ì €ìëª…ì„ ì œê³µí•˜ì„¸ìš”.**"""
        
        else:  # level - ë² ìŠ¤íŠ¸ì…€ëŸ¬ 3ê¶Œ ë°˜í™˜
            # ì•Œë¼ë”˜ APIë¡œ í•œêµ­ì–´ êµì¬ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê°€ì ¸ì˜¤ê¸°
            aladin_api_key = os.getenv('ALADIN_API_KEY')
            
            if not aladin_api_key:
                logger.warning("âš ï¸ ì•Œë¼ë”˜ API í‚¤ ì—†ìŒ - AI ì¶”ì²œ ì‚¬ìš©")
                # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ AI ì¶”ì²œ ì‚¬ìš©
                level_map = {
                    'ì´ˆê¸‰': "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
                    'ì¤‘ê¸‰': "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
                    'ê³ ê¸‰': "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
                }
                level_description = level_map.get(level, level)
                
                prompt = f"""í•œêµ­ì–´ í•™ìŠµ ë„ì„œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ ì¶œíŒëœ ìœ ëª…í•œ í•œêµ­ì–´ í•™ìŠµ êµì¬ì˜ ì •í™•í•œ ì œëª©ê³¼ ì €ìë¥¼ ì œê³µí•˜ì„¸ìš”.

{level_description} í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì±… 5ê¶Œì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì±…ë§Œ ì¶”ì²œí•˜ì„¸ìš”:**
- ì„œìš¸ëŒ€ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì—°ì„¸ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì´í™” í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- Korean Grammar in Use ì‹œë¦¬ì¦ˆ
- ê·¸ ì™¸ ê²€ì¦ëœ í•œêµ­ì–´ í•™ìŠµ êµì¬

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"books": [{"title": "ì •í™•í•œ ì±… ì œëª© (ì¶œíŒì‚¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •í™•í•œ ì œëª©)", "author": "ì €ìëª…", "isbn13": "ISBN-13 ë²ˆí˜¸ (í•˜ì´í”ˆ í¬í•¨, ì˜ˆ: 978-89-1234-567-8)", "description": "ì´ ì±…ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ  (2-3ë¬¸ì¥)"}]}

**ISBN-13ì„ ëª¨ë¥´ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‘ë˜, ë°˜ë“œì‹œ ì •í™•í•œ ì±… ì œëª©ê³¼ ì €ìëª…ì„ ì œê³µí•˜ì„¸ìš”.**"""
            else:
                # ì•Œë¼ë”˜ ê²€ìƒ‰ APIë¡œ ë ˆë²¨ë³„ í•œêµ­ì–´ êµì¬ ê²€ìƒ‰ í›„ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆœìœ¼ë¡œ ì •ë ¬
                try:
                    # ë ˆë²¨ë³„ ê²€ìƒ‰ì–´ ì„¤ì •
                    level_keywords = {
                        'ì´ˆê¸‰': ['ì´ˆê¸‰ í•œêµ­ì–´', 'í•œêµ­ì–´ ì…ë¬¸', 'í•œêµ­ì–´ ê¸°ì´ˆ', 'TOPIK 1ê¸‰', 'TOPIK 2ê¸‰', 'í•œêµ­ì–´ 1ë‹¨ê³„'],
                        'ì¤‘ê¸‰': ['ì¤‘ê¸‰ í•œêµ­ì–´', 'í•œêµ­ì–´ ì¤‘ê¸‰', 'TOPIK 3ê¸‰', 'TOPIK 4ê¸‰', 'í•œêµ­ì–´ 2ë‹¨ê³„', 'í•œêµ­ì–´ 3ë‹¨ê³„'],
                        'ê³ ê¸‰': ['ê³ ê¸‰ í•œêµ­ì–´', 'í•œêµ­ì–´ ê³ ê¸‰', 'TOPIK 5ê¸‰', 'TOPIK 6ê¸‰', 'í•œêµ­ì–´ 4ë‹¨ê³„', 'í•œêµ­ì–´ 5ë‹¨ê³„']
                    }
                    
                    # ì„ íƒëœ ë ˆë²¨ì— ë§ëŠ” ê²€ìƒ‰ì–´ ì‚¬ìš©
                    search_queries = level_keywords.get(level, ['í•œêµ­ì–´ êµì¬', 'í•œêµ­ì–´ í•™ìŠµ', 'TOPIK êµì¬'])
                    
                    # ë ˆë²¨ë³„ í•„í„°ë§ í‚¤ì›Œë“œ (ì œëª©ì— í¬í•¨ë˜ì–´ì•¼ í•¨)
                    level_filter_keywords = {
                        'ì´ˆê¸‰': ['ì´ˆê¸‰', 'ì…ë¬¸', 'ê¸°ì´ˆ', '1ê¸‰', '2ê¸‰', '1ë‹¨ê³„', '2ë‹¨ê³„', 'beginner', 'basic'],
                        'ì¤‘ê¸‰': ['ì¤‘ê¸‰', '3ê¸‰', '4ê¸‰', '2ë‹¨ê³„', '3ë‹¨ê³„', 'intermediate'],
                        'ê³ ê¸‰': ['ê³ ê¸‰', '5ê¸‰', '6ê¸‰', '4ë‹¨ê³„', '5ë‹¨ê³„', 'advanced']
                    }
                    
                    filter_keywords = level_filter_keywords.get(level, [])
                    
                    all_results = []
                    seen_series = set()  # ì‹œë¦¬ì¦ˆëª… ì¶”ì ìš©
                    
                    def extract_series_name(title):
                        """ì œëª©ì—ì„œ ì‹œë¦¬ì¦ˆëª… ì¶”ì¶œ (ì˜ˆ: 'ì„œê°• í•œêµ­ì–´ 1A', 'ì—°ì„¸ í•œêµ­ì–´ 1-1' ë“±)"""
                        import re
                        # ì¼ë°˜ì ì¸ í•œêµ­ì–´ êµì¬ ì‹œë¦¬ì¦ˆ íŒ¨í„´
                        patterns = [
                            r'([ê°€-í£]+ í•œêµ­ì–´\s*\d+[A-Z]?)',  # "ì„œê°• í•œêµ­ì–´ 1A", "ì—°ì„¸ í•œêµ­ì–´ 1"
                            r'([ê°€-í£]+ í•œêµ­ì–´\s*\d+-\d+)',   # "ì—°ì„¸ í•œêµ­ì–´ 1-1"
                            r'(ì„œìš¸ëŒ€ í•œêµ­ì–´\s*\d+)',         # "ì„œìš¸ëŒ€ í•œêµ­ì–´ 1"
                            r'(ì´í™” í•œêµ­ì–´\s*\d+)',           # "ì´í™” í•œêµ­ì–´ 1"
                            r'(Korean Grammar in Use)',       # "Korean Grammar in Use"
                            r'(Talk To Me In Korean)',        # "Talk To Me In Korean"
                        ]
                        
                        for pattern in patterns:
                            match = re.search(pattern, title, re.IGNORECASE)
                            if match:
                                series = match.group(1).strip()
                                # ìˆ«ìì™€ ë‹¨ê³„ ì •ë³´ ì œê±°í•˜ì—¬ ì‹œë¦¬ì¦ˆëª…ë§Œ ì¶”ì¶œ
                                # ì˜ˆ: "ì„œê°• í•œêµ­ì–´ 1A" -> "ì„œê°• í•œêµ­ì–´"
                                series_base = re.sub(r'\s*\d+[A-Z]?.*$', '', series)
                                series_base = re.sub(r'\s*\d+-\d+.*$', '', series_base)
                                return series_base.strip()
                        return None
                    
                    # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ ìˆ˜ì§‘
                    for query in search_queries:
                        search_url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
                        search_params = {
                            'ttbkey': aladin_api_key,
                            'Query': query,
                            'QueryType': 'Title',
                            'MaxResults': '10',
                            'start': '1',
                            'SearchTarget': 'Book',
                            'output': 'js',
                            'Version': '20131101',
                            'Sort': 'SalesPoint'  # íŒë§¤ëŸ‰ ìˆœ ì •ë ¬
                        }
                        
                        logger.info(f"ğŸ“š ì•Œë¼ë”˜ ê²€ìƒ‰: '{query}' (íŒë§¤ëŸ‰ ìˆœ)")
                        search_response = requests.get(search_url, params=search_params, timeout=10)
                        search_response.raise_for_status()
                        search_data = search_response.json()
                        
                        if search_data.get('item'):
                            # í•œêµ­ì–´ êµì¬ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                            keywords = ['í•œêµ­ì–´', 'TOPIK', 'KOREAN', 'í•œê¸€', 'ë¬¸ë²•', 'êµì¬', 'í•™ìŠµ', 'ì…ë¬¸', 'ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰']
                            
                            for item in search_data['item']:
                                title = item.get('title', '').upper()
                                original_title = item.get('title', '')
                                
                                # 1ë‹¨ê³„: í•œêµ­ì–´ êµì¬ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                                if not any(keyword.upper() in title for keyword in keywords):
                                    continue
                                
                                # 2ë‹¨ê³„: ë ˆë²¨ë³„ í•„í„°ë§ (ì œëª©ì— ë ˆë²¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸)
                                # ë‹¨, ë ˆë²¨ í‚¤ì›Œë“œê°€ ì—†ì–´ë„ ê¸°ë³¸ í•œêµ­ì–´ êµì¬ëŠ” í¬í•¨ (ë„ˆë¬´ ì œí•œì ì´ì§€ ì•Šë„ë¡)
                                title_lower = original_title.lower()
                                has_level_keyword = any(kw.lower() in title_lower for kw in filter_keywords)
                                
                                # ë ˆë²¨ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë ˆë²¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸ (ì œì™¸)
                                other_level_keywords = []
                                for other_level, other_keywords in level_filter_keywords.items():
                                    if other_level != level:
                                        other_level_keywords.extend(other_keywords)
                                
                                has_other_level_keyword = any(kw.lower() in title_lower for kw in other_level_keywords)
                                
                                # ë‹¤ë¥¸ ë ˆë²¨ í‚¤ì›Œë“œê°€ ëª…í™•íˆ ìˆìœ¼ë©´ ì œì™¸
                                if has_other_level_keyword and not has_level_keyword:
                                    logger.info(f"â­ï¸ ë ˆë²¨ ë¶ˆì¼ì¹˜ ìŠ¤í‚µ: {original_title} (ë‹¤ë¥¸ ë ˆë²¨ í‚¤ì›Œë“œ í¬í•¨)")
                                    continue
                                
                                # ì¤‘ë³µ ì œê±° (ISBN ê¸°ì¤€)
                                isbn = item.get('isbn13', item.get('isbn', ''))
                                cleaned_isbn = isbn.replace('-', '') if isbn else ''
                                
                                if cleaned_isbn and not any(r.get('isbn') == cleaned_isbn for r in all_results):
                                    # ì‹œë¦¬ì¦ˆëª… ì¶”ì¶œ
                                    series_name = extract_series_name(original_title)
                                    
                                    # ê°™ì€ ì‹œë¦¬ì¦ˆê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ (íŒë§¤ëŸ‰ì´ ë” ë†’ì€ ê²ƒë§Œ ìœ ì§€)
                                    if series_name:
                                        if series_name in seen_series:
                                            # ì´ë¯¸ ê°™ì€ ì‹œë¦¬ì¦ˆê°€ ìˆìœ¼ë©´ íŒë§¤ëŸ‰ ë¹„êµ
                                            existing_item = next((r for r in all_results if extract_series_name(r['item'].get('title', '')) == series_name), None)
                                            if existing_item:
                                                current_sales = int(item.get('salesPoint', 0))
                                                existing_sales = existing_item['sales_point']
                                                if current_sales <= existing_sales:
                                                    logger.info(f"â­ï¸ ì‹œë¦¬ì¦ˆ ì¤‘ë³µ ìŠ¤í‚µ: {original_title} (ì‹œë¦¬ì¦ˆ: {series_name}, íŒë§¤ëŸ‰: {current_sales} <= {existing_sales})")
                                                    continue
                                                else:
                                                    # ë” ë†’ì€ íŒë§¤ëŸ‰ì˜ ì±…ìœ¼ë¡œ êµì²´
                                                    logger.info(f"ğŸ”„ ì‹œë¦¬ì¦ˆ êµì²´: {original_title} (ì‹œë¦¬ì¦ˆ: {series_name}, íŒë§¤ëŸ‰: {current_sales} > {existing_sales})")
                                                    all_results = [r for r in all_results if extract_series_name(r['item'].get('title', '')) != series_name]
                                        else:
                                            seen_series.add(series_name)
                                    
                                    all_results.append({
                                        'item': item,
                                        'sales_point': int(item.get('salesPoint', 0)),
                                        'isbn': cleaned_isbn
                                    })
                    
                    # íŒë§¤ëŸ‰ ìˆœìœ¼ë¡œ ì •ë ¬
                    all_results.sort(key=lambda x: x['sales_point'], reverse=True)
                    
                    if len(all_results) > 0:
                        # ìƒìœ„ 3ê¶Œ ë°˜í™˜
                        books = []
                        for result in all_results[:3]:
                            item = result['item']
                            isbn13 = item.get('isbn13', item.get('isbn', ''))
                            cleaned_isbn = isbn13.replace('-', '') if isbn13 else ''
                            
                            cover_image = item.get('cover', '')
                            if cover_image and not cover_image.startswith('http'):
                                cover_image = f"https://image.aladin.co.kr/product/{cover_image}"
                            
                            # ë ˆë²¨ì— ë§ëŠ” ì„¤ëª… ìƒì„±
                            level_descriptions = {
                                'ì´ˆê¸‰': f"ì´ˆê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ êµì¬ì…ë‹ˆë‹¤. ê¸°ì´ˆ ë¬¸ë²•ê³¼ ì–´íœ˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                                'ì¤‘ê¸‰': f"ì¤‘ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ êµì¬ì…ë‹ˆë‹¤. ì‹¤ìš©ì ì¸ í‘œí˜„ê³¼ ë¬¸ë²•ì„ ì‹¬í™” í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                                'ê³ ê¸‰': f"ê³ ê¸‰ í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ êµì¬ì…ë‹ˆë‹¤. ê³ ê¸‰ ë¬¸ë²•ê³¼ í‘œí˜„ì„ ë§ˆìŠ¤í„°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                            }
                            
                            books.append({
                                "id": cleaned_isbn or f"{item.get('title', '')}-{item.get('author', '')}",
                                "title": item.get('title', ''),
                                "author": item.get('author', ''),
                                "description": level_descriptions.get(level, "í•œêµ­ì–´ í•™ìŠµì„ ìœ„í•œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ êµì¬ì…ë‹ˆë‹¤."),
                                "coverImageUrl": cover_image,
                                "isbn": cleaned_isbn,
                                "publisher": item.get('publisher', ''),
                                "pubdate": item.get('pubDate', ''),
                                "price": item.get('priceStandard', 0),
                                "link": item.get('link', '')
                            })
                        
                        logger.info(f"âœ… í•œêµ­ì–´ êµì¬ ë² ìŠ¤íŠ¸ì…€ëŸ¬ {len(books)}ê¶Œ ë°˜í™˜")
                        return jsonify(books)
                    else:
                        logger.warning("âš ï¸ í•œêµ­ì–´ êµì¬ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - AI ì¶”ì²œìœ¼ë¡œ fallback")
                        raise Exception("í•œêµ­ì–´ êµì¬ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                        
                except Exception as e:
                    logger.warning(f"ì•Œë¼ë”˜ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}, AI ì¶”ì²œìœ¼ë¡œ fallback")
                    # ì•Œë¼ë”˜ API ì‹¤íŒ¨ ì‹œ AI ì¶”ì²œìœ¼ë¡œ fallback
                    level_map = {
                        'ì´ˆê¸‰': "ì´ˆê¸‰ (TOPIK 1-2ê¸‰)",
                        'ì¤‘ê¸‰': "ì¤‘ê¸‰ (TOPIK 3-4ê¸‰)",
                        'ê³ ê¸‰': "ê³ ê¸‰ (TOPIK 5-6ê¸‰)"
                    }
                    level_description = level_map.get(level, level)
                    
                    prompt = f"""í•œêµ­ì–´ í•™ìŠµ ë„ì„œ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œ ì¶œíŒëœ ìœ ëª…í•œ í•œêµ­ì–´ í•™ìŠµ êµì¬ì˜ ì •í™•í•œ ì œëª©ê³¼ ì €ìë¥¼ ì œê³µí•˜ì„¸ìš”.

{level_description} í•œêµ­ì–´ í•™ìŠµìë¥¼ ìœ„í•œ ì±… 5ê¶Œì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

**ë°˜ë“œì‹œ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì±…ë§Œ ì¶”ì²œí•˜ì„¸ìš”:**
- ì„œìš¸ëŒ€ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì—°ì„¸ í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- ì´í™” í•œêµ­ì–´ ì‹œë¦¬ì¦ˆ
- Korean Grammar in Use ì‹œë¦¬ì¦ˆ
- ê·¸ ì™¸ ê²€ì¦ëœ í•œêµ­ì–´ í•™ìŠµ êµì¬

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"books": [{"title": "ì •í™•í•œ ì±… ì œëª© (ì¶œíŒì‚¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •í™•í•œ ì œëª©)", "author": "ì €ìëª…", "isbn13": "ISBN-13 ë²ˆí˜¸ (í•˜ì´í”ˆ í¬í•¨, ì˜ˆ: 978-89-1234-567-8)", "description": "ì´ ì±…ì„ ì¶”ì²œí•˜ëŠ” ì´ìœ  (2-3ë¬¸ì¥)"}]}

**ISBN-13ì„ ëª¨ë¥´ë©´ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë‘ë˜, ë°˜ë“œì‹œ ì •í™•í•œ ì±… ì œëª©ê³¼ ì €ìëª…ì„ ì œê³µí•˜ì„¸ìš”.**"""
        
        logger.info(f"ğŸ“š Geminiì— ì±… ì¶”ì²œ ìš”ì²­: type={recommend_type}, level={level}, mood={mood}")
        
        # Gemini API í˜¸ì¶œ
        try:
            response = model.generate_content(prompt)
            response_text = response.text
            logger.info(f"Gemini ì‘ë‹µ ë°›ìŒ: {len(response_text)}ì")
        except Exception as e:
            logger.error(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}", exc_info=True)
            return jsonify({"error": f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}), 500
        
        # JSON ì¶”ì¶œ (```json ``` ì œê±°)
        import json
        import re
        
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response_text)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_match = re.search(r'```\s*([\s\S]*?)\s*```', response_text)
                if json_match:
                    json_str = json_match.group(1)
                else:
                    json_str = response_text
            
            books_data = json.loads(json_str)
            books = books_data.get('books', [])
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸ ì‘ë‹µ: {response_text[:500]}", exc_info=True)
            return jsonify({"error": f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}), 500
        
        if not books:
            return jsonify({"error": "ì±… ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        logger.info(f"âœ… Gemini ì¶”ì²œ ì™„ë£Œ: {len(books)}ê¶Œ")
        
        # ì•Œë¼ë”˜ APIë¡œ ê° ì±…ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        aladin_api_key = os.getenv('ALADIN_API_KEY')
        
        if not aladin_api_key:
            logger.warning("âš ï¸ ì•Œë¼ë”˜ API í‚¤ ì—†ìŒ - AI ì¶”ì²œë§Œ ë°˜í™˜")
            return jsonify(books)
        
        enriched_books = []
        
        def normalize_string(s):
            """ë¬¸ìì—´ ì •ê·œí™” (ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)"""
            if not s:
                return ""
            return s.replace(' ', '').replace('ã€€', '').lower()
        
        def search_aladin(query, query_type='Title'):
            """ì•Œë¼ë”˜ API ê²€ìƒ‰"""
            search_url = "http://www.aladin.co.kr/ttb/api/ItemSearch.aspx"
            search_params = {
                'ttbkey': aladin_api_key,
                'Query': query,
                'QueryType': query_type,
                'MaxResults': '10',  # ë” ë§ì€ ê²°ê³¼ í™•ì¸
                'start': '1',
                'SearchTarget': 'Book',
                'output': 'js',
                'Version': '20131101'
            }
            try:
                search_response = requests.get(search_url, params=search_params, timeout=10)
                search_response.raise_for_status()
                return search_response.json()
            except Exception as e:
                logger.warning(f"ì•Œë¼ë”˜ ê²€ìƒ‰ ì‹¤íŒ¨ ({query}): {str(e)}")
                return None
        
        for book in books:
            title = book.get('title', '').strip()
            author = book.get('author', '').strip()
            ai_description = book.get('description', '')
            gemini_isbn = book.get('isbn13', '').strip()  # Geminiê°€ ì œê³µí•œ ISBN
            
            found = False
            
            # Geminiê°€ ISBNì„ ì œê³µí•œ ê²½ìš°, ì§ì ‘ ItemLookUp ì‹œë„
            if gemini_isbn:
                cleaned_isbn = gemini_isbn.replace('-', '').replace(' ', '')
                if cleaned_isbn and len(cleaned_isbn) >= 10:
                    try:
                        detail_url = "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
                        detail_params = {
                            'ttbkey': aladin_api_key,
                            'itemIdType': 'ISBN13' if len(cleaned_isbn) == 13 else 'ISBN',
                            'ItemId': cleaned_isbn,
                            'output': 'js',
                            'Version': '20131101',
                            'Cover': 'Big'
                        }
                        
                        detail_response = requests.get(detail_url, params=detail_params, timeout=10)
                        detail_response.raise_for_status()
                        detail_data = detail_response.json()
                        
                        if detail_data.get('item') and len(detail_data['item']) > 0:
                            detail_item = detail_data['item'][0]
                            enriched_books.append({
                                "id": cleaned_isbn,
                                "title": detail_item.get('title', title),
                                "author": detail_item.get('author', author),
                                "description": ai_description,
                                "coverImageUrl": detail_item.get('cover', ''),
                                "isbn": cleaned_isbn,
                                "publisher": detail_item.get('publisher', ''),
                                "pubdate": detail_item.get('pubDate', ''),
                                "price": detail_item.get('priceStandard', 0),
                                "link": detail_item.get('link', '')
                            })
                            logger.info(f"âœ… ISBNìœ¼ë¡œ ì§ì ‘ ì¡°íšŒ ì„±ê³µ: {title} (ISBN: {cleaned_isbn})")
                            found = True
                            continue
                    except Exception as e:
                        logger.warning(f"ISBN ì§ì ‘ ì¡°íšŒ ì‹¤íŒ¨ ({title}, ISBN: {gemini_isbn}): {str(e)}")
                        # ISBN ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback
            
            # ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹œë„ (ë” ë§ì€ ë³€í˜• ì‹œë„)
            search_queries = []
            if title:
                search_queries.append((title, 'Title'))
                # ì œëª©ì—ì„œ ê´„í˜¸ ë‚´ìš© ì œê±° í›„ ê²€ìƒ‰
                title_clean = title.split('(')[0].split('[')[0].split('ã€')[0].split('ã€‘')[0].strip()
                if title_clean != title and title_clean:
                    search_queries.append((title_clean, 'Title'))
                # ì œëª©ì˜ ì²« ë¶€ë¶„ë§Œ ì‚¬ìš© (ê¸´ ì œëª©ì˜ ê²½ìš°)
                title_first = title.split(' ')[0] if ' ' in title else title
                if len(title_first) > 3 and title_first != title:
                    search_queries.append((title_first, 'Title'))
            if author:
                search_queries.append((author, 'Author'))
                # ì €ìëª…ì˜ ì²« ë¶€ë¶„ë§Œ ì‚¬ìš©
                author_first = author.split(' ')[0] if ' ' in author else author
                if author_first != author:
                    search_queries.append((author_first, 'Author'))
            if title and author:
                search_queries.append((f"{title} {author}", 'Title'))
                search_queries.append((f"{author} {title}", 'Title'))
            
            best_match = None
            best_score = 0
            
            # ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œë„
            for query, query_type in search_queries:
                if found:
                    break
                    
                search_data = search_aladin(query, query_type)
                
                if search_data and search_data.get('item') and len(search_data['item']) > 0:
                    normalized_title = normalize_string(title)
                    normalized_author = normalize_string(author) if author else ""
                    
                    # ìµœì  ë§¤ì¹­ ì°¾ê¸° (ë” ì •êµí•œ ì ìˆ˜ ê³„ì‚°)
                    for item in search_data['item']:
                        item_title = normalize_string(item.get('title', ''))
                        item_author = normalize_string(item.get('author', ''))
                        
                        score = 0
                        # ì œëª© ë§¤ì¹­ ì ìˆ˜
                        if normalized_title:
                            if normalized_title == item_title:
                                score += 50  # ì™„ì „ ì¼ì¹˜
                            elif normalized_title in item_title:
                                score += 30  # ë¶€ë¶„ ì¼ì¹˜ (ì œëª©ì´ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨)
                            elif item_title in normalized_title:
                                score += 20  # ë¶€ë¶„ ì¼ì¹˜ (ê²€ìƒ‰ ê²°ê³¼ê°€ ì œëª©ì— í¬í•¨)
                            else:
                                # ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­
                                title_words = set(normalized_title.replace('(', '').replace(')', '').split())
                                item_title_words = set(item_title.replace('(', '').replace(')', '').split())
                                common_words = title_words & item_title_words
                                if common_words:
                                    score += len(common_words) * 5
                        
                        # ì €ì ë§¤ì¹­ ì ìˆ˜
                        if normalized_author and item_author:
                            if normalized_author in item_author or item_author in normalized_author:
                                score += 15
                        
                        if score > best_score:
                            best_score = score
                            best_match = item
                    
                    # ì¶©ë¶„í•œ ì ìˆ˜ë¥¼ ì–»ìœ¼ë©´ ì‚¬ìš© (ì„ê³„ê°’ ë‚®ì¶¤)
                    if best_score >= 10:  # ìµœì†Œ ì„ê³„ê°’ ë‚®ì¶¤ (20 -> 10)
                        found = True
                        break
            
            # ë§¤ì¹­ëœ ì±… ì •ë³´ ì¶”ì¶œ
            if best_match and found:
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì§ì ‘ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ItemLookUp í˜¸ì¶œ ì—†ì´)
                isbn13 = best_match.get('isbn13', '')
                isbn = best_match.get('isbn', '')
                final_isbn = isbn13 if isbn13 else isbn
                cleaned_isbn = final_isbn.replace('-', '') if final_isbn else ''
                
                cover_image = best_match.get('cover', '')
                # ì»¤ë²„ ì´ë¯¸ì§€ê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì „ì²´ URLë¡œ ë³€í™˜
                if cover_image and not cover_image.startswith('http'):
                    cover_image = f"https://image.aladin.co.kr/product/{cover_image}"
                
                enriched_books.append({
                    "id": cleaned_isbn or f"{title}-{author}",
                    "title": best_match.get('title', title),
                    "author": best_match.get('author', author),
                    "description": ai_description,
                    "coverImageUrl": cover_image,
                    "isbn": cleaned_isbn,
                    "publisher": best_match.get('publisher', ''),
                    "pubdate": best_match.get('pubDate', ''),
                    "price": best_match.get('priceStandard', 0),
                    "link": best_match.get('link', '')
                })
                logger.info(f"âœ… ì•Œë¼ë”˜ ì •ë³´ ì¶”ê°€: {title} (ì ìˆ˜: {best_score})")
            else:
                # ì•Œë¼ë”˜ì—ì„œ ëª» ì°¾ìœ¼ë©´ ë„¤ì´ë²„ API ì‹œë„
                naver_client_id = os.getenv('NAVER_CLIENT_ID')
                naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
                
                if naver_client_id and naver_client_secret:
                    try:
                        naver_query = f"{title} {author}".strip()
                        naver_url = "https://openapi.naver.com/v1/search/book.json"
                        naver_params = {'query': naver_query, 'display': 1, 'sort': 'sim'}
                        naver_headers = {
                            'X-Naver-Client-Id': naver_client_id,
                            'X-Naver-Client-Secret': naver_client_secret
                        }
                        
                        naver_response = requests.get(naver_url, params=naver_params, headers=naver_headers, timeout=5)
                        naver_response.raise_for_status()
                        naver_data = naver_response.json()
                        
                        if naver_data.get('items') and len(naver_data['items']) > 0:
                            naver_item = naver_data['items'][0]
                            enriched_books.append({
                                "id": f"{title}-{author}",
                                "title": naver_item.get('title', '').replace('<b>', '').replace('</b>', ''),
                                "author": naver_item.get('author', author),
                                "description": ai_description,
                                "coverImageUrl": naver_item.get('image', ''),
                                "isbn": naver_item.get('isbn', ''),
                                "publisher": naver_item.get('publisher', ''),
                                "pubdate": naver_item.get('pubdate', ''),
                                "price": naver_item.get('price', 0),
                                "link": naver_item.get('link', '')
                            })
                            logger.info(f"âœ… ë„¤ì´ë²„ ì •ë³´ ì¶”ê°€: {title}")
                            continue
                    except Exception as e:
                        logger.warning(f"ë„¤ì´ë²„ ê²€ìƒ‰ ì‹¤íŒ¨ ({title}): {str(e)}")
                
                # ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ì •ë³´ë§Œ
                enriched_books.append({
                    "id": f"{title}-{author}",
                    "title": title,
                    "author": author,
                    "description": ai_description,
                    "coverImageUrl": None,
                    "isbn": None,
                    "publisher": None,
                    "pubdate": None,
                    "price": None,
                    "link": None
                })
                logger.warning(f"âš ï¸ ì•Œë¼ë”˜/ë„¤ì´ë²„ì—ì„œ ì°¾ì§€ ëª»í•¨: '{title}' (ì €ì: {author}) - ì‹œë„í•œ ì¿¼ë¦¬: {[q[0] for q in search_queries]}")
        
        logger.info(f"ğŸ‰ ìµœì¢… ê²°ê³¼: {len(enriched_books)}ê¶Œ (ì•Œë¼ë”˜ ì •ë³´ í¬í•¨)")
        return jsonify(enriched_books)
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}), 500
    except Exception as e:
        logger.error(f"ì±… ì¶”ì²œ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return jsonify({"error": f"ì±… ì¶”ì²œ ì‹¤íŒ¨: {str(e)}"}), 500

if __name__ == '__main__':
    # ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
    logger.info("=" * 60)
    logger.info("ğŸš€ í•œê¸€ì •ì› AI ë°±ì—”ë“œ ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info("=" * 60)
    
    logger.info("\n[1/4] EasyOCR í•œê¸€ ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_easyocr_reader()
    
    logger.info("\n[2/4] TrOCR ì†ê¸€ì”¨ ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_model()
    
    logger.info("\n[3/4] Whisper ìŒì„± ì¸ì‹ ëª¨ë¸ ë¡œë”©...")
    load_whisper_model()
    
    logger.info("\n[4/4] ko-sroberta-multitask ë¬¸ì¥ ìœ ì‚¬ë„ ëª¨ë¸ ë¡œë”©...")
    load_sroberta_model()
    
    logger.info("\n" + "=" * 60)
    logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    logger.info("âœ¨ ì†ê¸€ì”¨ ì¸ì‹: EasyOCR (í•œê¸€ íŠ¹í™”)")
    logger.info("ğŸ¯ ë¬¸ì¥ ìœ ì‚¬ë„ ëª¨ë¸: ko-sroberta-multitask (Mean Pooling)")
    logger.info("ğŸ“Š ë²¤ì¹˜ë§ˆí¬: í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ 1ìœ„")
    logger.info("ğŸ”¥ ì •í™•ë„: 92%+ (ê¸°ì¡´ ëŒ€ë¹„ +3% í–¥ìƒ)")
    logger.info("âš¡ ì†ë„: 2ë°° í–¥ìƒ (ë‹¨ì¼ ëª¨ë¸)")
    logger.info("=" * 60 + "\n")
    
    # Flask ì„œë²„ ì‹¤í–‰
    # RailwayëŠ” PORT í™˜ê²½ ë³€ìˆ˜ë¥¼ ì œê³µí•˜ë¯€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 5001
    port = int(os.getenv('PORT', 5001))
    app.run(host='0.0.0.0', port=port, debug=False)




